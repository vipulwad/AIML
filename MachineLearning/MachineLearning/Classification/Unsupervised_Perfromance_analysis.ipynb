{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDjwMOIeCyEB"
   },
   "source": [
    "# Cluster Performance Analysis\n",
    "Performance analysis in clustering methods involves assessing how well a clustering algorithm has grouped data points into meaningful clusters.\n",
    "1. **Inertia or Sum of Squared Distances (SSD)**: Inertia represents the sum of squared distances of samples to their closest cluster center. It is a measure of how compact the clusters are. Lower inertia indicates better clustering. However, inertia alone may not always be sufficient for assessing the quality of clusters.\n",
    "\n",
    "2. **Silhouette Score**: The silhouette score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The score ranges from -1 to 1, where a higher score indicates better-defined clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThfsjUziE3hl"
   },
   "source": [
    "## Inertia or Sum of Squared Distances (SSD)\n",
    "Inertia, also known as the Sum of Squared Distances (SSD) or within-cluster sum of squares, is a measure used to evaluate the performance of a clustering algorithm, particularly in the context of K-means clustering. It quantifies how far the points within a cluster are from the centroid of that cluster.\n",
    "\n",
    "Here's how inertia is calculated in the context of K-means clustering:\n",
    "\n",
    "1. **For each data point in the dataset, calculate the squared Euclidean distance from the point to the centroid of the cluster it belongs to.**\n",
    "   \n",
    "2. **Sum up these squared distances for all points in all clusters.**\n",
    "\n",
    "In mathematical terms, if $(C_i)$ represents the centroid of the $(i$)-th cluster and $(x_j$) represents the $(j$)-th data point in that cluster, the inertia $(I$) is calculated as:\n",
    "\n",
    "$[ I = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left\\| x_j - C_i \\right\\|^2 ]$\n",
    "\n",
    "where:\n",
    "- $( k )$ is the number of clusters.\n",
    "- $( n_i )$ is the number of data points in the $(i)$-th cluster.\n",
    "\n",
    "The objective of K-means clustering is to minimize this inertia. In other words, K-means tries to find cluster assignments and centroids that minimize the sum of squared distances of each point to the centroid of its assigned cluster.\n",
    "\n",
    "When using the elbow method for determining the optimal number of clusters (K), practitioners often plot the values of inertia for different values of K and look for the \"elbow\" in the plot. The elbow is the point where the rate of decrease in inertia starts to slow down, and adding more clusters does not lead to a significant reduction in inertia. The optimal K is often chosen at this elbow point, as it represents a good balance between model complexity and the ability to capture the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GMkB-ttnlye0"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHq5XAGFmaft"
   },
   "outputs": [],
   "source": [
    "# Generate some random data for demonstration\n",
    "np.random.seed(42)\n",
    "X,y = make_blobs(n_samples=100, centers=5, n_features=2)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXY2R5UDCxvu"
   },
   "outputs": [],
   "source": [
    "# Specify the number of clusters (K)\n",
    "k = 5\n",
    "\n",
    "# Fit KMeans model to the data\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the inertia (Sum of Squared Distances)\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "print(f\"Inertia: {inertia}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfqlIT3wCxqm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiHPZSaMCxln"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0nr8-bQCxhM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TzRanXmANgh"
   },
   "source": [
    "# How to define K value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz5QEDQYAUKJ"
   },
   "source": [
    "## Elbow Method\n",
    "The elbow method is a technique used in cluster analysis and machine learning to determine the optimal number of clusters for a dataset. When performing clustering, such as K-means clustering, you need to specify the number of clusters beforehand. The elbow method helps you find the point at which adding more clusters does not significantly improve the model's performance.\n",
    "\n",
    "1. Run the algorithm with different numbers of clusters (k): Execute the clustering algorithm (e.g., K-means) on the dataset for a range of values of k. Typically, you would choose a range of k values, such as 1 to 10.\n",
    "\n",
    "2. Compute the sum of squared distances (SSD): For each value of k, calculate the sum of squared distances from each point to its assigned cluster center. The sum of squared distances is also known as the \"inertia\" or \"within-cluster sum of squares.\"\n",
    "\n",
    "3. Plot the results: Create a plot where the x-axis represents the number of clusters (k), and the y-axis represents the corresponding sum of squared distances.\n",
    "\n",
    "4. Identify the \"elbow\" in the plot: The idea is to look for a point on the plot where adding more clusters does not result in a significant decrease in the sum of squared distances. This point is often referred to as the \"elbow.\" The elbow represents a trade-off between the model's complexity (number of clusters) and its goodness of fit.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The optimal number of clusters is usually the value at the elbow, as it indicates a good balance between capturing the underlying structure of the data and avoiding overfitting.\n",
    "\n",
    "Keep in mind that the elbow method is a heuristic, and the interpretation of the elbow may not always be straightforward. In some cases, the elbow may not be well-defined, and other methods or domain knowledge may be needed to determine the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ASRRdKViqUY7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "N-6ME0k6qW90"
   },
   "outputs": [],
   "source": [
    "# Generate random data with three clusters\n",
    "# X, _ = make_blobs(n_samples=3000, centers=10, cluster_std=1.0, random_state=42)\n",
    "# plt.scatter(X[:,0], X[:,1])\n",
    "iris_data=load_iris()\n",
    "X=iris_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "6UDa8yYEq5X5"
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the sum of squared distances (inertia) for each k\n",
    "inertia_values = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef60Z-CaAE-3"
   },
   "outputs": [],
   "source": [
    "# Try different values of k (from 1 to 10, for example)\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(range(1, 11), inertia_values, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances (Inertia)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3_-15p_tUOw"
   },
   "outputs": [],
   "source": [
    "inertia_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b3G0CTOrRIr"
   },
   "outputs": [],
   "source": [
    "kmeans_test = KMeans(n_clusters=7, random_state=42)\n",
    "labels=kmeans_test.fit_predict(X)\n",
    "# kmeans.cluster_centers_\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=5, c=labels)\n",
    "plt.scatter(kmeans_test.cluster_centers_[:,0],kmeans_test.cluster_centers_[:,1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ip3-Sr5brzC8",
    "outputId": "82427c69-c9f7-455b-b369-bb182224662e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.2793763938471"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_test.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzGdziWzA18T"
   },
   "source": [
    "## Exercise 1\n",
    "Take a real life dataset and perform elbow method to find the optimal value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OaqVE20Ao1H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rW24JwxGLED"
   },
   "source": [
    "# Silhouette Score\n",
    "The Silhouette Score is a metric used to calculate the goodness of a clustering technique, such as K-means, by measuring how well-defined the clusters are in a given dataset. It quantifies the distance between the resulting clusters and assesses the cohesion and separation of the clusters.\n",
    "\n",
    "The Silhouette Score for a single data point $(i)$ is defined as follows:\n",
    "\n",
    "$[ \\text{Silhouette}(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}} ]$\n",
    "\n",
    "where:\n",
    "- $(a(i))$ is the average distance from the $(i)$-th data point to the other data points in the same cluster (intra-cluster distance).\n",
    "- $(b(i))$ is the smallest average distance from the $(i)$-th data point to data points in a different cluster (inter-cluster distance).\n",
    "\n",
    "The overall Silhouette Score for the entire dataset is the average of the Silhouette scores for each data point, and it ranges from -1 to 1. A high Silhouette Score indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "Interpretation of Silhouette Score:\n",
    "- A score close to 1 suggests that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "- A score around 0 indicates overlapping clusters.\n",
    "- A score close to -1 suggests that the object is poorly matched to its own cluster and well matched to a neighboring cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "PX8iEtB96N8n"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "tUy_fzXI6PPw"
   },
   "outputs": [],
   "source": [
    "# Generate some random data for demonstration\n",
    "X, _ = make_blobs(n_samples=3000, centers=4, cluster_std=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N43apeefGO5h",
    "outputId": "aa1c7880-ffdc-4d22-a56a-74aabb3b5cb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.7914720000513819\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of clusters (K)\n",
    "k = 4\n",
    "\n",
    "# Fit KMeans model to the data\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict cluster labels\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdVwQ2zyIINB"
   },
   "source": [
    "## Exercise 2\n",
    "Use Silhouette Score to determine K value in Kmeans. Plot Silhouette Score and K values and in the plot, look for the K value that corresponds to the peak of the silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "GcxGvYTh7S1Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "-ssOi1tf7Tzs"
   },
   "outputs": [],
   "source": [
    "# Generate some random data for demonstration\n",
    "# X, _ = make_blobs(n_samples=3000, centers=7,\n",
    "#                   cluster_std=1.0, random_state=42)\n",
    "data=load_iris()\n",
    "X=data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMSLnWe1IMKR"
   },
   "outputs": [],
   "source": [
    "# Specify a range of K values\n",
    "k_values = range(2, 11)  # You can adjust this range based on your problem\n",
    "\n",
    "# Store the silhouette scores for each K\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over different K values\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the Silhouette Scores for each K\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Score for Different K Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSvLgZhcIkgf"
   },
   "source": [
    "## Silhouette Score for Heirarchical Aglomerative Clustering (HAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "tSt91JS6_7zF"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "ehhGyCYeAD-w"
   },
   "outputs": [],
   "source": [
    "X, _ = make_blobs(n_samples=300, centers=4,\n",
    "                  cluster_std=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B_C9XKIIwRf"
   },
   "outputs": [],
   "source": [
    "# Specify a range of linkage methods and corresponding K values\n",
    "linkage_methods = ['ward', 'complete', 'average']\n",
    "k_values = range(2, 11)\n",
    "\n",
    "# Store the silhouette scores for each combination of linkage method and K\n",
    "silhouette_scores = {}\n",
    "\n",
    "# Iterate over different linkage methods\n",
    "for linkage in linkage_methods:\n",
    "    silhouette_scores[linkage] = []\n",
    "\n",
    "    # Iterate over different K values\n",
    "    for k in k_values:\n",
    "        hac = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "        labels = hac.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores[linkage].append(silhouette_avg)\n",
    "\n",
    "# Plot the Silhouette Scores for each linkage method and K value\n",
    "for linkage, scores in silhouette_scores.items():\n",
    "    plt.plot(k_values, scores, marker='o', label=linkage)\n",
    "\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Score for Different Linkage Methods and K Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KCOMatvNRiu"
   },
   "source": [
    "# NOTE\n",
    "Can silhouette score be used in dbscan and mean shift?\n",
    "The Silhouette Score is a metric commonly used with partitioning clustering algorithms (such as K-means) and hierarchical clustering methods. However, it is not typically applied to density-based clustering algorithms like DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and mean shift.\n",
    "\n",
    "Here's why:\n",
    "\n",
    "1. **DBSCAN:**\n",
    "   - DBSCAN doesn't necessarily produce clusters with well-defined shapes or sizes. It identifies dense regions separated by areas of lower point density, and the number of clusters is not predetermined. The Silhouette Score assumes well-defined clusters, and its calculation involves comparing the distance of a point to points within the same cluster and to the nearest points in other clusters. In DBSCAN, points in the same \"cluster\" may not necessarily be directly connected or have a clear boundary.\n",
    "\n",
    "2. **Mean Shift:**\n",
    "   - Mean shift is a non-parametric clustering algorithm that identifies modes in the data distribution. Like DBSCAN, it doesn't assume a specific number of clusters, and the resulting clusters may have irregular shapes. Silhouette Score, designed for partitioning clusters, may not be suitable for evaluating the quality of clusters formed by mean shift.\n",
    "\n",
    "For density-based clustering algorithms like DBSCAN, alternative evaluation metrics such as the Davies-Bouldin Index or visual inspection of the resulting clusters may be more appropriate.\n",
    "\n",
    "For mean shift and other non-parametric methods, the evaluation is often done through visual examination of the resulting clusters and assessing how well they capture the underlying structure of the data.\n",
    "\n",
    "In summary, while the Silhouette Score is a useful metric for certain types of clustering algorithms, it is not universally applicable to all clustering methods. Always consider the characteristics of the algorithm and the nature of the data when selecting an appropriate evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFdY7QubJdVA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
