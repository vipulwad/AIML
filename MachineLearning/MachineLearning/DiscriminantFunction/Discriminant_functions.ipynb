{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsG0aT5dmFkU"
   },
   "source": [
    "## what is Discriminant Functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZsBsPt6msod"
   },
   "source": [
    "Discriminant functions and decision rules are fundamental concepts in the context of classification problems. Let's break down each term:\n",
    "\n",
    "### Discriminant Function:\n",
    "A discriminant function is a mathematical function that takes input features and maps them to a decision or classification. The primary purpose of the discriminant function is to discriminate between different classes or categories. The form of the discriminant function depends on the specific classification algorithm being used. Here are a few examples:\n",
    "\n",
    "1. **Linear Discriminant Function (LDF):** In linear discriminant analysis (LDA), the discriminant function is a linear combination of the input features. It can be represented as $ Y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b $, where $\\mathbf{w}$ is a weight vector, $\\mathbf{x}$ is the input feature vector, and $b$ is a bias term.\n",
    "\n",
    "2. **Quadratic Discriminant Function (QDF):** In quadratic discriminant analysis (QDA), the discriminant function involves quadratic terms in addition to linear terms, providing more flexibility.\n",
    "\n",
    "3. **Support Vector Machines (SVM):** SVMs use a discriminant function based on a hyperplane that maximally separates different classes in the feature space. The decision boundary is determined by support vectors.\n",
    "\n",
    "4. **Logistic Regression:** In logistic regression, the logistic or sigmoid function is used as the discriminant function to model the probability of belonging to a particular class.\n",
    "\n",
    "### Decision Rule:\n",
    "The decision rule is a criterion or condition based on the output of the discriminant function that determines the final class assignment. It's the rule that specifies how to make decisions or predictions based on the computed discriminant values. The decision rule typically involves comparing the output of the discriminant function to a threshold or using some criteria to assign the input to a specific class.\n",
    "\n",
    "For example, in a binary classification problem:\n",
    "\n",
    "- If $ Y(\\mathbf{x}) > \\text{Threshold} $, assign the input to Class 1.\n",
    "- If $ Y(\\mathbf{x}) \\leq \\text{Threshold} $, assign the input to Class 2.\n",
    "\n",
    "The choice of the decision rule can impact the performance of the classification model, and it may be adjusted based on the specific requirements of the application.\n",
    "\n",
    "In summary, discriminant functions provide a way to map input features to decision values, and decision rules determine how those values are translated into class assignments. These concepts are foundational in the field of pattern recognition, machine learning, and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6Frpr9EnyLP"
   },
   "source": [
    "In the context of Gaussian distribution and discriminant functions, there are three common cases that are used in pattern recognition and statistical classification:\n",
    "\n",
    "1. **Single-Class (Single Gaussian) Discriminant Function:($C_1=C_2=C$)**\n",
    "   - **Assumption:** All classes share a common covariance matrix.\n",
    "   - **Discriminant Function:** Assumes that the covariance matrix is the same for all classes $( \\Sigma_1 = \\Sigma_2 = \\ldots = \\Sigma_k )$.\n",
    "   - **Considerations**:\n",
    "    The data for different classes exhibit similar statistical properties.\n",
    "    The covariance matrix is assumed to be the same across all classes.\n",
    "   - **Decision Rule:** Assign the input to the class with the highest posterior probability given by the Gaussian distribution.\n",
    "\n",
    "2. **Diagonal Covariance Matrix Discriminant Function: ($C_1=C_2=σ^2I$)**\n",
    "   - **Assumption:** Each class has its own diagonal covariance matrix (variances along the coordinate axes).\n",
    "   - **Discriminant Function:** Assumes that the covariance matrices are diagonal $\\Sigma_i = \\text{diag}(\\sigma_{i1}^2, \\sigma_{i2}^2, \\ldots, \\sigma_{ip}^2)$.\n",
    "   - **Considerations**:\n",
    "    The variances of the features (dimensions) for each class might be different.\n",
    "    The correlations between different features are assumed to be negligible.\n",
    "   - **Decision Rule:** Similar to the single-class case, the decision rule involves assigning the input to the class with the highest posterior probability given by the Gaussian distribution.\n",
    "\n",
    "3. **General Covariance Matrix Discriminant Function:($C_1 \\neq C_2$)**\n",
    "   - **Assumption:** Each class has its own general (non-diagonal) covariance matrix.\n",
    "   - **Discriminant Function:** Does not assume that covariance matrices are the same or diagonal.\n",
    "   - **Considerations**:\n",
    "    The variances of the features may differ between classes.\n",
    "    The correlations between different features can be taken into account.\n",
    "    Provides more flexibility in modeling the shape of the distribution for each class.\n",
    "   - **Decision Rule:** The decision rule involves calculating the Mahalanobis distance, which accounts for the correlations between different features. The input is assigned to the class with the smallest Mahalanobis distance.\n",
    "\n",
    "In all cases, the discriminant functions are derived from the assumption that the data in each class follows a multivariate Gaussian (normal) distribution. These discriminant functions are used in techniques like Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) for classification tasks.\n",
    "\n",
    "It's important to note that the choice between these cases depends on the underlying assumptions about the distribution of the data and the characteristics of the classes being modeled. The single-class case is simpler and assumes a common covariance matrix for all classes. The diagonal and general covariance matrix cases provide more flexibility in modeling the shape of the distribution for each class. The appropriate choice often depends on the nature of the data and the specific requirements of the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oyYe7_Rs7mF"
   },
   "source": [
    "## Random Datapoint generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wc1IVpoomA3G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random data for two classes\n",
    "num_samples = 100\n",
    "mean_class1 = [1, 2]\n",
    "mean_class2 = [4, 3]\n",
    "covariance_class1 = [[1, 0.5], [0.5, 1]]\n",
    "covariance_class2 = [[1, -0.8], [-0.8, 1]]\n",
    "\n",
    "class1_data = np.random.multivariate_normal(mean_class1, covariance_class1, num_samples)\n",
    "class2_data = np.random.multivariate_normal(mean_class2, covariance_class2, num_samples)\n",
    "\n",
    "# Combine the data\n",
    "all_data = np.vstack([class1_data, class2_data])\n",
    "labels = np.hstack([np.zeros(num_samples), np.ones(num_samples)])\n",
    "\n",
    "# Plot the generated data\n",
    "plt.scatter(class1_data[:, 0], class1_data[:, 1], label='Class 1', marker='o')\n",
    "plt.scatter(class2_data[:, 0], class2_data[:, 1], label='Class 2', marker='x')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Generated 2D Random Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8Kik7LkDmgW"
   },
   "source": [
    "## Case 1 : Single-Class (Single Gaussian) Discriminant Function:($C_1=C_2=C$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKAsMtNuDl1z"
   },
   "outputs": [],
   "source": [
    "# Calculate mean and covariance for all data\n",
    "mean_all = np.mean(all_data, axis=0)\n",
    "covariance_all = np.cov(all_data, rowvar=False)\n",
    "print(mean_all, covariance_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXN66QgKD4UB"
   },
   "outputs": [],
   "source": [
    "# Create a multivariate normal distribution using mean and covariance\n",
    "distribution_all = multivariate_normal(mean_all, covariance_all)\n",
    "distribution_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMyJrT7mEqol"
   },
   "outputs": [],
   "source": [
    "# Discriminant function for single-class case\n",
    "def single_class_discriminant_function(x):\n",
    "    return distribution_all.pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_VDG-dHEwQm"
   },
   "outputs": [],
   "source": [
    "# Predict the class for a new data point\n",
    "new_point = np.array([5, 5])\n",
    "prediction = single_class_discriminant_function(new_point)\n",
    "\n",
    "print(\"Discriminant Function Value for the New Data Point:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk6pbEpSsZ_2"
   },
   "outputs": [],
   "source": [
    "# Decision Rule: Assign the input to the class with the highest posterior probability\n",
    "if prediction<0.005:\n",
    "  predicted_class=2\n",
    "else:\n",
    "  predicted_class=1\n",
    "print(\"Predicted Class for the New Data Point:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owHi6FXdFQzB"
   },
   "outputs": [],
   "source": [
    "# Plot the generated data\n",
    "plt.scatter(class1_data[:, 0], class1_data[:, 1], label='Class 1', marker='o')\n",
    "plt.scatter(class2_data[:, 0], class2_data[:, 1], label='Class 2', marker='x')\n",
    "plt.scatter(new_point[0], new_point[1], label='New Point', marker='^')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Generated 2D Random Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLD6L-R0FQkZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cBHRdyxG4Rn"
   },
   "source": [
    "## Case-2 : Diagonal Covariance Matrix Discriminant Function: ($C_1=C_2=σ^2I$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVnsOO6RHMlH"
   },
   "outputs": [],
   "source": [
    "class1_data=all_data[:100, :]\n",
    "class2_data=all_data[100: :]\n",
    "\n",
    "class1_data.shape, class2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-uL5H82IFPY"
   },
   "outputs": [],
   "source": [
    "# Calculate mean and covariance for each class separately\n",
    "mean_class1 = np.mean(class1_data, axis=0)\n",
    "mean_class2 = np.mean(class2_data, axis=0)\n",
    "covariance_class1 = np.diag(np.var(class1_data, axis=0))\n",
    "covariance_class2 = np.diag(np.var(class2_data, axis=0))\n",
    "\n",
    "print(mean_class1, covariance_class1)\n",
    "print(mean_class2, covariance_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiq-jAaVIcvm"
   },
   "outputs": [],
   "source": [
    "# Create multivariate normal distributions for each class\n",
    "distribution_class1 = multivariate_normal(mean_class1, covariance_class1)\n",
    "distribution_class2 = multivariate_normal(mean_class2, covariance_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpDwTjEOIh5m"
   },
   "outputs": [],
   "source": [
    "# Discriminant function for diagonal covariance matrix case\n",
    "def diagonal_covariance_discriminant_function(x):\n",
    "    return distribution_class1.pdf(x), distribution_class2.pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOdpTuVCIm68"
   },
   "outputs": [],
   "source": [
    "# Decision Rule: Assign the input to the class with the highest posterior probability\n",
    "def predict_class(x):\n",
    "    pdf_class1, pdf_class2 = diagonal_covariance_discriminant_function(x)\n",
    "    if pdf_class1>pdf_class2:\n",
    "      return 1, pdf_class1\n",
    "    else:\n",
    "      return 2, pdf_class2\n",
    "    # return 0 if pdf_class1 > pdf_class2 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvEmdOqbsy4r"
   },
   "outputs": [],
   "source": [
    "# Predict the class for a new data point\n",
    "new_point = np.array([1, 1])\n",
    "predicted_class, predicted_probability = predict_class(new_point)\n",
    "print(\"Predicted Class for the New Data Point:\", predicted_class)\n",
    "print(\"Prdicted Class probability\", predicted_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iak1YAIdI5vd"
   },
   "outputs": [],
   "source": [
    "# Plot the generated data\n",
    "plt.scatter(class1_data[:, 0], class1_data[:, 1], label='Class 1', marker='o')\n",
    "plt.scatter(class2_data[:, 0], class2_data[:, 1], label='Class 2', marker='x')\n",
    "plt.scatter(new_point[0], new_point[1], label='New Point', marker='^')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Generated 2D Random Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN985OSOI88V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oobokb3fI85V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTu6JpsOI82P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPA5NuykKKA2"
   },
   "source": [
    "## Case 3 : General Covariance Matrix Discriminant Function:($C_1 \\neq C_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hOCqOt0KYqz",
    "outputId": "e995e3ed-7dfb-443c-98f3-201a70be5453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100, 2))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1_data=all_data[:100, :]\n",
    "class2_data=all_data[100: :]\n",
    "\n",
    "class1_data.shape, class2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvBGHnAUKYq0",
    "outputId": "24a0fa2f-3a5f-4879-b6f6-246b45c6b3db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.08307042 2.11709274] [[0.8150929  0.        ]\n",
      " [0.         0.76768711]]\n",
      "[3.89208458 3.13541943] [[1.09769667 0.        ]\n",
      " [0.         1.00118854]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and covariance for each class separately\n",
    "mean_class1 = np.mean(class1_data, axis=0)\n",
    "mean_class2 = np.mean(class2_data, axis=0)\n",
    "covariance_class1 = np.diag(np.var(class1_data, axis=0))\n",
    "covariance_class2 = np.diag(np.var(class2_data, axis=0))\n",
    "\n",
    "print(mean_class1, covariance_class1)\n",
    "print(mean_class2, covariance_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89Twat2NtvbN"
   },
   "outputs": [],
   "source": [
    "# Create multivariate normal distributions for each class\n",
    "distribution_class1 = multivariate_normal(mean_class1, covariance_class1)\n",
    "distribution_class2 = multivariate_normal(mean_class2, covariance_class2)\n",
    "\n",
    "# Discriminant function for general covariance matrix case\n",
    "def general_covariance_discriminant_function(x):\n",
    "    return distribution_class1.pdf(x), distribution_class2.pdf(x)\n",
    "\n",
    "# Decision Rule: Assign the input to the class with the smallest Mahalanobis distance\n",
    "def predict_class_case3(x):\n",
    "    pdf_class1, pdf_class2 = general_covariance_discriminant_function(x)\n",
    "    return 0 if pdf_class1 > pdf_class2 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEofLlRbKh3H",
    "outputId": "9cddcd75-f6b6-41bf-f25c-583ce9b235fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class for the New Data Point: 1\n"
     ]
    }
   ],
   "source": [
    "# Predict the class for a new data point\n",
    "new_point = np.array([3, 3.5])\n",
    "predicted_class = predict_class_case3(new_point)\n",
    "\n",
    "print(\"Predicted Class for the New Data Point:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AApYOAq6KnkY"
   },
   "outputs": [],
   "source": [
    "# Plot the generated data\n",
    "plt.scatter(class1_data[:, 0], class1_data[:, 1], label='Class 1', marker='o')\n",
    "plt.scatter(class2_data[:, 0], class2_data[:, 1], label='Class 2', marker='x')\n",
    "plt.scatter(new_point[0], new_point[1], label='New Point', marker='^')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Generated 2D Random Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyEllLL7MFw9"
   },
   "source": [
    "## Testing Dataset with CASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUaXmH70KoYk"
   },
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "mean_class1 = [1, 2]\n",
    "mean_class2 = [6, 7]\n",
    "covariance_class1 = [[1, 0.5], [0.5, 1]]\n",
    "covariance_class2 = [[1, -0.8], [-0.8, 1]]\n",
    "\n",
    "class1_data = np.random.multivariate_normal(mean_class1, covariance_class1, num_samples)\n",
    "class2_data = np.random.multivariate_normal(mean_class2, covariance_class2, num_samples)\n",
    "\n",
    "# Combine the data\n",
    "test_all_data = np.vstack([class1_data, class2_data])\n",
    "test_labels = np.hstack([np.zeros(num_samples), np.ones(num_samples)])\n",
    "\n",
    "# Plot the generated data\n",
    "plt.scatter(test_all_data[:20, 0], test_all_data[:20, 1], label='Class 1', marker='o')\n",
    "plt.scatter(test_all_data[20:, 0], test_all_data[20:, 1], label='Class 2', marker='x')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Generated 2D Random Test Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dknhxYqMLurZ"
   },
   "outputs": [],
   "source": [
    "pred_label=[]\n",
    "pred_prob=[]\n",
    "for d in test_all_data:\n",
    "  pred=predict_class_case3(d)\n",
    "  # print(pred)\n",
    "  pred_label.append(pred)\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZhLFEO1MaQm"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBD278dDM0wd",
    "outputId": "0d68a73c-4385-472c-9de3-4a0cc6822bee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=metrics.accuracy_score(test_labels, pred_label)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhDZSaqNM9SL",
    "outputId": "713ddb5f-dd6d-42ba-8824-b421197a6c9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1=metrics.f1_score(test_labels, pred_label)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqXkjZfYNFAH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
