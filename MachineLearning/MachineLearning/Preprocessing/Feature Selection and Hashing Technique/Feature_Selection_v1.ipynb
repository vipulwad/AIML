{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpyWuVipPx96"
   },
   "source": [
    "# <center> Feature Selection </center>\n",
    "\n",
    "Feature selection is one of the important preprocessing steps in any machine learning task. A feature in case of a dataset simply means a column. When we get any dataset, not necessarily every column (feature) is going to have an impact on the output variable. If we add these irrelevant features in the model, it will just make the model performance worse (i.e Garbage In Garbage Out). This gives rise to the need of doing feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zs0TNQlDiWRc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YjeykS5f8Wt"
   },
   "source": [
    "# Forward Feature Selection:\n",
    "\n",
    "* Forward feature selection starts with an empty set of features and iteratively adds the most relevant feature at each step.\n",
    "* The algorithm evaluates each feature's performance individually using a chosen evaluation metric (e.g., accuracy, F1-score).\n",
    "* It selects the feature that provides the best improvement in the evaluation metric and adds it to the selected features set.\n",
    "* This process continues until a stopping criterion is met (e.g., a predefined number of features are selected or the performance stops improving).\n",
    "\n",
    "Forward feature selection is typically used when you have a large pool of potential features, and you want to identify the most relevant subset for your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GugSpQHNgQd8"
   },
   "source": [
    "# Backward Feature Selection:\n",
    "\n",
    "* Backward feature selection starts with all available features and iteratively removes the least relevant feature at each step.\n",
    "* The algorithm evaluates the model's performance after removing each feature and selects the feature whose removal causes the least decrease in the evaluation metric.\n",
    "* This process continues until a stopping criterion is met (e.g., a predefined number of features remain or the performance starts degrading significantly)\n",
    "\n",
    "Backward feature selection is often used when you have a large number of features and you want to simplify the model or reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZAHdc6VAgQBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41999526  0.38087892  1.05803637  0.05871946 -1.06514246]\n",
      " [-0.70561162  1.01345596  0.66848172  1.23482107 -0.78757691]\n",
      " [-1.2300397   0.56989039 -0.84436956  1.30745517  0.72045114]\n",
      " ...\n",
      " [-0.12468906 -1.41093398  0.99229634 -2.68524513 -0.73675032]\n",
      " [ 0.5895268  -0.3869628   1.24062782 -1.22303666 -1.12534416]\n",
      " [-1.0680554  -1.48418409  0.69252136 -2.64815779 -0.44010265]] [0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0\n",
      " 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1\n",
      " 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
      " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
      " 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
      " 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1\n",
      " 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
      " 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
      " 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0\n",
      " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
      " 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1\n",
      " 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
      " 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a toy dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, random_state=3)\n",
    "print (X, y)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an empty set to store selected features\n",
    "selected_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wU-j0GsXf74l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selected Features: [4, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Perform Forward Feature Selection\n",
    "while len(selected_features) < X_train.shape[1]:\n",
    "    best_feature = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for feature_idx in range(X_train.shape[1]):\n",
    "        if feature_idx not in selected_features:\n",
    "            # Combine the currently selected features with the new candidate\n",
    "            features_to_use = selected_features + [feature_idx]\n",
    "\n",
    "            # Train a model using only the selected features\n",
    "            model = LogisticRegression(random_state=42)\n",
    "            model.fit(X_train[:, features_to_use], y_train)\n",
    "\n",
    "            # Make predictions on the test set\n",
    "            y_pred = model.predict(X_test[:, features_to_use])\n",
    "\n",
    "            # Evaluate the model's accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(accuracy)\n",
    "\n",
    "            # Update the best feature if the accuracy is higher\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_feature = feature_idx\n",
    "\n",
    "    # Add the best feature to the selected features\n",
    "    selected_features.append(best_feature)\n",
    "\n",
    "print(\"Forward Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aZX0XIUaei16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Selected Features: [3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Perform Backward Feature Selection\n",
    "selected_features = list(range(X_train.shape[1]))  # Start with all features\n",
    "\n",
    "while len(selected_features) > 1:\n",
    "    worst_feature = None\n",
    "    worst_accuracy = 1.0\n",
    "\n",
    "    for feature_idx in selected_features:\n",
    "        # Create a list of features excluding the current one\n",
    "        features_to_use = [f for f in selected_features if f != feature_idx]\n",
    "\n",
    "        # Train a model using the selected features\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        model.fit(X_train[:, features_to_use], y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test[:, features_to_use])\n",
    "\n",
    "        # Evaluate the model's accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Update the worst feature if the accuracy is lower\n",
    "        if accuracy < worst_accuracy:\n",
    "            worst_accuracy = accuracy\n",
    "            worst_feature = feature_idx\n",
    "\n",
    "    # Remove the worst feature from the selected features\n",
    "    selected_features.remove(worst_feature)\n",
    "\n",
    "print(\"Backward Selected Features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
