{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5giaFNDuhkawXKio22w1n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# One-hot vector\n","A one-hot vector is a binary representation used to represent categorical data where each bit in the vector represents a category, and only one bit is set to 1 (hot), while all others are 0 (cold). In the context of Natural Language Processing (NLP), one-hot vectors are often used to represent words or tokens in a vocabulary."],"metadata":{"id":"-54tqrXaAE00"}},{"cell_type":"markdown","source":["* High Dimensionality:\n","\n","  One-hot encoding results in vectors that are very high-dimensional, especially when the vocabulary size is large. Each unique token or word in the vocabulary is represented by a vector of zeros with only one element set to 1. This can lead to very sparse representations and increased computational complexity.\n","\n","* Lack of Semantic Information:\n","\n","  One-hot vectors do not capture any semantic relationships or similarities between words. Each word is represented as a unique entity, with no information about its context or meaning. This makes it challenging for models to generalize and understand similarities between related words.\n","\n","* Memory and Computational Efficiency:\n","\n","  One-hot vectors are not memory-efficient, especially for large vocabularies. Storing and manipulating high-dimensional sparse matrices can be computationally expensive and inefficient, both in terms of memory usage and computational operations.\n","\n","* Not Suitable for Sequential Models:\n","\n","  For tasks where sequence or order matters, such as in natural language processing tasks like sequence prediction or language modeling, one-hot vectors do not encode any sequential information. They treat each word independently, ignoring the order and context in which words appear.\n","\n","* Curse of Dimensionality:\n","\n","  One-hot encoding exacerbates the curse of dimensionality, particularly in high-dimensional spaces. As the number of unique tokens increases, the vector space grows exponentially, leading to increased computational requirements and potential overfitting in models.\n","\n","* No Embedding of Similarity:\n","\n","  There is no inherent embedding of similarity between words. In contrast, dense embeddings like Word2Vec, GloVe, or FastText embed words into continuous vector spaces where distances (e.g., cosine similarity) can reflect semantic similarity between words.\n","\n","* Model Sparsity:\n","\n","  Models trained on one-hot vectors can suffer from sparsity issues. Sparse representations may not adequately capture relationships between words, making it harder for models to generalize effectively, especially on tasks requiring understanding of natural language semantics."],"metadata":{"id":"01jQmBkmB1E7"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFpVNky8-hQ8","executionInfo":{"status":"ok","timestamp":1718993629265,"user_tz":-330,"elapsed":10,"user":{"displayName":"Pankaj Choudhury","userId":"11381187948817891789"}},"outputId":"db9cc149-74d6-40e8-8c2c-a8020e7e706a"},"outputs":[{"output_type":"stream","name":"stdout","text":["One-hot vector for 'banana':\n","[0 1 0 0]\n"]}],"source":["import numpy as np\n","\n","# Sample vocabulary\n","vocabulary = ['apple', 'banana', 'cherry', 'date']\n","\n","# Example word to encode\n","word = 'banana'\n","\n","# Create one-hot vector for the word\n","def one_hot_encode(word, vocabulary):\n","    vector = np.zeros(len(vocabulary), dtype=int)\n","    index = vocabulary.index(word)\n","    vector[index] = 1\n","    return vector\n","\n","# Encode the example word\n","one_hot_vector = one_hot_encode(word, vocabulary)\n","\n","# Print the result\n","print(f\"One-hot vector for '{word}':\")\n","print(one_hot_vector)\n"]},{"cell_type":"markdown","source":["# Co-occurrence Matrix\n","A Term Document Matrix (TDM) is a matrix representation of a collection of documents where each row corresponds to a term (word) in the vocabulary and each column corresponds to a document. The entries in the matrix represent the frequency of the terms in the documents. This is useful for various NLP tasks such as document classification, clustering, and information retrieval.\n","\n"],"metadata":{"id":"w0yK1ss5Ajij"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","# Sample documents\n","documents = [\n","    \"This is the first document.\",\n","    \"This document is the second document.\",\n","    \"And this is the third one.\",\n","    \"Is this the first document?\"\n","]\n","\n","# Create the Term Document Matrix using CountVectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the matrix to a pandas DataFrame for better visualization\n","df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","print(\"Term Document Matrix:\")\n","print(df)\n","\n","# Extract document vectors\n","document_vectors = X.toarray()\n","for i, doc_vector in enumerate(document_vectors):\n","    print(f\"Document {i+1} vector: {doc_vector}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9wk0opzAQqA","executionInfo":{"status":"ok","timestamp":1718993962667,"user_tz":-330,"elapsed":3006,"user":{"displayName":"Pankaj Choudhury","userId":"11381187948817891789"}},"outputId":"4dfc7719-2fc5-41b7-b515-8ed94b05bdad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Term Document Matrix:\n","   and  document  first  is  one  second  the  third  this\n","0    0         1      1   1    0       0    1      0     1\n","1    0         2      0   1    0       1    1      0     1\n","2    1         0      0   1    1       0    1      1     1\n","3    0         1      1   1    0       0    1      0     1\n","Document 1 vector: [0 1 1 1 0 0 1 0 1]\n","Document 2 vector: [0 2 0 1 0 1 1 0 1]\n","Document 3 vector: [1 0 0 1 1 0 1 1 1]\n","Document 4 vector: [0 1 1 1 0 0 1 0 1]\n"]}]},{"cell_type":"markdown","source":["# Term Frequency-Inverse Document Frequency\n","TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that is intended to reflect the importance of a word in a document relative to a collection of documents (or corpus). It is often used as a weighting factor in text mining and information retrieval.\n","\n","Let's walk through a step-by-step example of computing TF-IDF scores using a small dataset.\n","\n","### Example Dataset\n","\n","Consider a small corpus with three documents:\n","\n","1. Document 1: \"the cat sat on the mat\"\n","2. Document 2: \"the cat sat\"\n","3. Document 3: \"the dog ate the bone\"\n","\n","### Step-by-Step Calculation\n","\n","1. **Term Frequency (TF):**\n","   TF measures how frequently a term appears in a document. The simplest way to calculate TF is just to use the raw count of the term in the document.\n","\n","   $\n","   \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n","   $\n","\n","   Let's calculate TF for each term in each document:\n","\n","   - Document 1: \"the cat sat on the mat\"\n","     - TF(\"the\", Document 1) = 2/6\n","     - TF(\"cat\", Document 1) = 1/6\n","     - TF(\"sat\", Document 1) = 1/6\n","     - TF(\"on\", Document 1) = 1/6\n","     - TF(\"mat\", Document 1) = 1/6\n","\n","   - Document 2: \"the cat sat\"\n","     - TF(\"the\", Document 2) = 1/3\n","     - TF(\"cat\", Document 2) = 1/3\n","     - TF(\"sat\", Document 2) = 1/3\n","\n","   - Document 3: \"the dog ate the bone\"\n","     - TF(\"the\", Document 3) = 2/5\n","     - TF(\"dog\", Document 3) = 1/5\n","     - TF(\"ate\", Document 3) = 1/5\n","     - TF(\"bone\", Document 3) = 1/5\n","\n","2. **Inverse Document Frequency (IDF):**\n","   IDF measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms, like \"is\", \"of\", and \"that\", may appear a lot but have little importance. Thus, we need to weigh down the frequent terms while scaling up the rare ones.\n","\n","   $\n","   \\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term } t} \\right)\n","   $\n","\n","   Let's calculate IDF for each term in the corpus:\n","\n","   - IDF(\"the\") = \\(\\log(3/3) = \\log(1) = 0\\)\n","   - IDF(\"cat\") = \\(\\log(3/2)\\)\n","   - IDF(\"sat\") = \\(\\log(3/2)\\)\n","   - IDF(\"on\") = \\(\\log(3/1)\\)\n","   - IDF(\"mat\") = \\(\\log(3/1)\\)\n","   - IDF(\"dog\") = \\(\\log(3/1)\\)\n","   - IDF(\"ate\") = \\(\\log(3/1)\\)\n","   - IDF(\"bone\") = \\(\\log(3/1)\\)\n","\n","   Using \\(\\log\\) base 10 for simplicity:\n","\n","   - IDF(\"cat\") = \\(\\log_{10}(1.5) \\approx 0.176\\)\n","   - IDF(\"sat\") = \\(\\log_{10}(1.5) \\approx 0.176\\)\n","   - IDF(\"on\") = \\(\\log_{10}(3) \\approx 0.477\\)\n","   - IDF(\"mat\") = \\(\\log_{10}(3) \\approx 0.477\\)\n","   - IDF(\"dog\") = \\(\\log_{10}(3) \\approx 0.477\\)\n","   - IDF(\"ate\") = \\(\\log_{10}(3) \\approx 0.477\\)\n","   - IDF(\"bone\") = \\(\\log_{10}(3) \\approx 0.477\\)\n","\n","3. **TF-IDF Calculation:**\n","   The TF-IDF score for a term is the product of its TF and IDF scores.\n","\n","   $\n","   \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n","   $\n","\n","   Let's calculate TF-IDF for each term in each document:\n","\n","   - Document 1: \"the cat sat on the mat\"\n","     - TF-IDF(\"the\", Document 1) = 2/6 * 0 = 0\n","     - TF-IDF(\"cat\", Document 1) = 1/6 * 0.176 ≈ 0.029\n","     - TF-IDF(\"sat\", Document 1) = 1/6 * 0.176 ≈ 0.029\n","     - TF-IDF(\"on\", Document 1) = 1/6 * 0.477 ≈ 0.080\n","     - TF-IDF(\"mat\", Document 1) = 1/6 * 0.477 ≈ 0.080\n","\n","   - Document 2: \"the cat sat\"\n","     - TF-IDF(\"the\", Document 2) = 1/3 * 0 = 0\n","     - TF-IDF(\"cat\", Document 2) = 1/3 * 0.176 ≈ 0.059\n","     - TF-IDF(\"sat\", Document 2) = 1/3 * 0.176 ≈ 0.059\n","\n","   - Document 3: \"the dog ate the bone\"\n","     - TF-IDF(\"the\", Document 3) = 2/5 * 0 = 0\n","     - TF-IDF(\"dog\", Document 3) = 1/5 * 0.477 ≈ 0.095\n","     - TF-IDF(\"ate\", Document 3) = 1/5 * 0.477 ≈ 0.095\n","     - TF-IDF(\"bone\", Document 3) = 1/5 * 0.477 ≈ 0.095\n","\n","### Summary of Results\n","\n","The TF-IDF scores for each term in each document are:\n","\n","- **Document 1:**\n","  - \"the\": 0\n","  - \"cat\": 0.029\n","  - \"sat\": 0.029\n","  - \"on\": 0.080\n","  - \"mat\": 0.080\n","\n","- **Document 2:**\n","  - \"the\": 0\n","  - \"cat\": 0.059\n","  - \"sat\": 0.059\n","\n","- **Document 3:**\n","  - \"the\": 0\n","  - \"dog\": 0.095\n","  - \"ate\": 0.095\n","  - \"bone\": 0.095\n","\n","These TF-IDF scores reflect the importance of each term within the document and the entire corpus, helping to identify which terms are most relevant to each document."],"metadata":{"id":"Xk7bNkhLEy_g"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","# Sample documents\n","documents = [\n","    \"the cat sat on the mat\",\n","    \"the cat sat\",\n","    \"the dog ate the bone\"\n","]\n","\n","# Create the TfidfVectorizer object\n","vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the documents\n","tfidf_matrix = vectorizer.fit_transform(documents)\n","\n","# Convert the TF-IDF matrix to a pandas DataFrame for better visualization\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Display the DataFrame\n","print(\"TF-IDF Matrix:\")\n","print(tfidf_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eW8Dli5jBjhh","executionInfo":{"status":"ok","timestamp":1718994935302,"user_tz":-330,"elapsed":488,"user":{"displayName":"Pankaj Choudhury","userId":"11381187948817891789"}},"outputId":"b587775a-9902-44e0-ac5a-9243c4a4c96a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Matrix:\n","        ate      bone       cat       dog       mat        on       sat  \\\n","0  0.000000  0.000000  0.356457  0.000000  0.468699  0.468699  0.356457   \n","1  0.000000  0.000000  0.619805  0.000000  0.000000  0.000000  0.619805   \n","2  0.476986  0.476986  0.000000  0.476986  0.000000  0.000000  0.000000   \n","\n","        the  \n","0  0.553642  \n","1  0.481334  \n","2  0.563431  \n"]}]},{"cell_type":"markdown","source":["## Limitation of TF-IDF\n","\n","* Lack of Context Understanding:\n","\n","  TF-IDF treats each term independently and does not capture the context in which terms appear. It ignores the order of words and relationships between them, which can be crucial for understanding the meaning of the text.\n","\n","* Sparse Representations:\n","\n","  TF-IDF vectors are often very high-dimensional and sparse, especially when dealing with large vocabularies. This can lead to inefficiencies in both storage and computation.\n","\n","* Limited to Bag-of-Words Model:\n","\n","  TF-IDF relies on the bag-of-words model, which disregards the grammar, syntax, and semantics of the language. This model counts word frequencies without considering word sequences or structures.\n","\n","* Sensitive to Frequent Terms:\n","\n","  While TF-IDF reduces the impact of very common words (like \"the\", \"is\", \"in\"), it still can be biased towards terms that appear frequently across documents but not frequently enough to be ignored completely. This can sometimes lead to less important terms being given more weight than they deserve.\n","\n","* Static Weighting Scheme:\n","\n","  The TF-IDF weighting scheme is static and does not adapt based on the specific task or domain. It uses a fixed formula to calculate term importance, which may not always align with the specific needs of a particular application.\n","\n","* Difficulty Handling Synonyms:\n","\n","  TF-IDF does not account for synonyms or different forms of the same word (e.g., \"run\" vs. \"running\"). Words with similar meanings are treated as completely separate features, which can dilute their significance.\n","\n","* Not Suitable for Small Corpora:\n","\n","  In smaller corpora, TF-IDF may not perform well because the inverse document frequency component can be less reliable when the number of documents is limited. The IDF values may be skewed due to the small sample size.\n","\n","* No Semantic Similarity:\n","\n","  TF-IDF vectors do not capture semantic similarity between words. For example, the words \"car\" and \"automobile\" will have completely different TF-IDF vectors, despite having similar meanings."],"metadata":{"id":"Yimsb2hRFl6B"}},{"cell_type":"code","source":[],"metadata":{"id":"QW9L9xY5FRl0"},"execution_count":null,"outputs":[]}]}